{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M2_4_Python_TextProcessing.ipynb","provenance":[{"file_id":"18p4RJuhav6j4T-2_aChK6VijKlnKhdgi","timestamp":1644472554474},{"file_id":"1maKLIjJUOAVmbQteUx2UQDbxCTlz0CKL","timestamp":1643870282628},{"file_id":"1MKe4fEYOOlXX4kEboLd26waFKCBf31RY","timestamp":1643347849446},{"file_id":"1m5X_Sv4YyUGo1G0PIOP27HKZEeWVuKkD","timestamp":1643268296128},{"file_id":"1_H9u1J0Ch1nBwRMfvwZj9SoywtNrPvf6","timestamp":1643091986472},{"file_id":"1K-pMrtjo9HLdUAN6KEaBEeaZ2tCISlN4","timestamp":1643016691281},{"file_id":"1JTUaGBOP7uinh9966vj0DcdWadEABgy2","timestamp":1642527290298},{"file_id":"1fAUsa8rI06aE3RlNfWysIkWQTuCl5WOJ","timestamp":1642505427550},{"file_id":"1lF_Co86RXrJA90HsesffPL2dRNCNtvbT","timestamp":1642501771605}],"collapsed_sections":["B5vDxy8beg9-","8DoFKy4-O2n3","Dooco0N9O6eY","R5fGrIQNfON1"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Module - 2 : Advanced Python** "],"metadata":{"id":"h9yTVQQIakWo"}},{"cell_type":"markdown","source":["Files in Python, Directories, Building Modules, Packages, **Text\n","Processing**, Regular expression in python"],"metadata":{"id":"JVmk7J4933mR"}},{"cell_type":"markdown","source":["# **Steps to Mount Google Drive in Google Colab**"],"metadata":{"id":"B5vDxy8beg9-"}},{"cell_type":"markdown","source":["[Documentation](https://neptune.ai/blog/google-colab-dealing-with-files) for mounting Google Drive in Google Colab"],"metadata":{"id":"z8w5hKE2S92N"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hqUi3AkURWu9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644569477657,"user_tz":-330,"elapsed":24600,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"b34cba3d-8b88-4eb7-ada0-6ef2dd647ee7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/\")\n","!ls"],"metadata":{"id":"2lK14e_8Ysa9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644569502373,"user_tz":-330,"elapsed":640,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"0b4d7586-e2c3-42b2-823e-081ae5f7e90c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MyDrive\n"]}]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/ColabNotebooks/Python_Colab_Notebooks/Python_File_Handling/\")\n","!ls"],"metadata":{"id":"EbVXI2KTY0ik","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644569505017,"user_tz":-330,"elapsed":1003,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"04465648-68c3-4338-d560-94dd81261faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["add.py\t\t hello.py  output2.txt\tsample_file_2.txt  trial123.txt\n","cars\t\t num2.py   output.txt\tsample_file2.txt\n","echo.py\t\t num2.pyc  poem.txt\tsample_file3.txt\n","file.py\t\t num.py    __pycache__\tsample_file.txt\n","File_Python.png  num.pyc   sample_file\tsample_package\n"]}]},{"cell_type":"code","source":["path_to_data = '/content/drive/MyDrive/ColabNotebooks/Python_Colab_Notebooks/Python_File_Handling'"],"metadata":{"id":"fnzx93BvTYV9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a file\n","!touch \"/content/drive/MyDrive/ColabNotebooks/Python_Colab_Notebooks/Python_File_Handling/sample_file3.txt\""],"metadata":{"id":"tpbjTly1fAn9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Unique_Labels_List = os.listdir(path_to_data)\n","print(Unique_Labels_List)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kM4wPUE-Tb09","executionInfo":{"status":"ok","timestamp":1643709873772,"user_tz":-330,"elapsed":499,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"e114fb02-5ca3-432e-9dd5-dc26b29099f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['sample_file2.txt', 'sample_file', 'File_Python.png', 'sample_file.txt', 'sample_file_2.txt', 'trial.txt', 'file2.txt', 'file.py', 'output.txt', 'sample_file3.txt']\n"]}]},{"cell_type":"markdown","source":["# **Text Processing in Python**"],"metadata":{"id":"8DoFKy4-O2n3"}},{"cell_type":"markdown","source":["* Text processing has a direct application to Natural Language Processing, also known as NLP. \n","* NLP is aimed at processing the languages spoken or written by humans when they communicate with one another\n","* [Applications of NLP](https://www.tutorialspoint.com/python_text_processing/python_text_processing_introduction.htm)\n","* text processing is essential to provide clean input for modelling and analysis."],"metadata":{"id":"t5Et90SoXS5T"}},{"cell_type":"markdown","source":["**Task - 1: String Immutability**"],"metadata":{"id":"vDZDIaLUYLlJ"}},{"cell_type":"code","source":["t= \"Hello Friends!!\"\n","print(type(t))\n","t[0] = \"M\""],"metadata":{"id":"yitVz9I0YSC8","colab":{"base_uri":"https://localhost:8080/","height":217},"executionInfo":{"status":"error","timestamp":1644573656154,"user_tz":-330,"elapsed":543,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"03781a28-328a-40e5-847c-cb21610869bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-763c1283a54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello Friends!!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"M\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"]}]},{"cell_type":"markdown","source":["**Task - 2 : Sorting Lines**"],"metadata":{"id":"GixoN1ILYgLJ"}},{"cell_type":"code","source":["file1 = open(\"poem.txt\", \"r+\")\n","data=file1.readlines()\n","for i in range(len(data)):\n","   print(data[i])"],"metadata":{"id":"6GgiMuWhYj0q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644573736279,"user_tz":-330,"elapsed":659,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"b182f771-0137-4fa1-d6c2-cf88c1d64028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Summer is here.\n","\n","  Sky is bright.\n","\n","\tBirds are gone.\n","\n","\t Nests are empty.\n","\n","\t  Where is Rain?\n","\n"]}]},{"cell_type":"code","source":["data.sort()\n","for i in range(len(data)):\n","    print(data[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAV-4dbQaFrE","executionInfo":{"status":"ok","timestamp":1644573742917,"user_tz":-330,"elapsed":380,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"f21fc96f-214d-4d35-aca6-0abff811fc18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\t  Where is Rain?\n","\n","\t Nests are empty.\n","\n","\tBirds are gone.\n","\n","  Sky is bright.\n","\n"," Summer is here.\n","\n"]}]},{"cell_type":"markdown","source":["**Task - 3 : Formatting Paragraphs**"],"metadata":{"id":"o03fAciOabD3"}},{"cell_type":"code","source":["!pip install textwrap3 "],"metadata":{"id":"VJ-VL7IfatZH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644569432982,"user_tz":-330,"elapsed":4611,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"4c9542d5-bbcd-4365-e9c3-9df4b47fe260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting textwrap3\n","  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n","Installing collected packages: textwrap3\n","Successfully installed textwrap3-0.9.2\n"]}]},{"cell_type":"code","source":["from textwrap3 import wrap\n","\n","text = 'In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones daughter Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia family, is known to friends and associates as Godfather. He and Tom Hagen (Robert Duvall), the Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, no Sicilian can refuse a request on his daughters wedding day.'\n","\n","x = wrap(text, 100)\n","for i in range(len(x)):\n","    print(x[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AP6HLHELao-T","executionInfo":{"status":"ok","timestamp":1644573845953,"user_tz":-330,"elapsed":384,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"d7f31d2d-effa-4c65-ba63-cccc16870a85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["In late summer 1945, guests are gathered for the wedding reception of Don Vito Corleones daughter\n","Connie (Talia Shire) and Carlo Rizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone\n","Mafia family, is known to friends and associates as Godfather. He and Tom Hagen (Robert Duvall), the\n","Corleone family lawyer, are hearing requests for favors because, according to Italian tradition, no\n","Sicilian can refuse a request on his daughters wedding day.\n"]}]},{"cell_type":"code","source":["import textwrap3\n","\n","file1 = open(\"poem.txt\", \"r+\")\n","\n","print(\"**Before Formatting**\")\n","print(\" \")\n","\n","data=file1.readlines()\n","for i in range(len(data)):\n","   print(data[i])\n","file1.close()  \n","\n","print(\" \")\n","print(\"**After Formatting**\")\n","print(\" \")\n","file1 = open(\"poem.txt\", \"r+\")\n","data=file1.readlines()\n","dedented_text = []\n","#print(data)\n","for i in range(len(data)):\n","  dedented_text = textwrap3.dedent(data[i]).strip()\n","  print(dedented_text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SXJnv78Ca8qk","executionInfo":{"status":"ok","timestamp":1644573937842,"user_tz":-330,"elapsed":570,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"096e588c-0ae8-4a4a-c2f0-4aa4585e930a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["**Before Formatting**\n"," \n"," Summer is here.\n","\n","  Sky is bright.\n","\n","\tBirds are gone.\n","\n","\t Nests are empty.\n","\n","\t  Where is Rain?\n","\n"," \n","**After Formatting**\n"," \n","Summer is here.\n","Sky is bright.\n","Birds are gone.\n","Nests are empty.\n","Where is Rain?\n"]}]},{"cell_type":"markdown","source":["**Task - 4 : Counting Token in Paragraphs**"],"metadata":{"id":"SnIGquNVBmY3"}},{"cell_type":"code","source":["with open('poem.txt', 'r') as file:\n","    lines_in_file = file.read()\n","    print(lines_in_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7uVNVXZBqzj","executionInfo":{"status":"ok","timestamp":1644570371715,"user_tz":-330,"elapsed":386,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"928b99a2-4162-4a24-8c87-871362f12c00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Summer is here.\n","  Sky is bright.\n","\tBirds are gone.\n","\t Nests are empty.\n","\t  Where is Rain?\n","\n"]}]},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPAHvg3uCoNH","executionInfo":{"status":"ok","timestamp":1644570524485,"user_tz":-330,"elapsed":3153,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"3516e7a5-f36e-4f10-fdb5-f077d1ea34b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}]},{"cell_type":"code","source":["# Counting Words Using nltk\n","# Please note the word 'here.' is counted as 2 words and not one\n","import nltk\n","nltk.download('punkt')\n","FileName = (\"poem.txt\")\n","\n","with open(FileName, 'r') as file:\n","    lines_in_file = file.read()\n","    \n","    nltk_tokens = nltk.word_tokenize(lines_in_file)\n","    print(nltk_tokens)\n","    print(\"\\n\")\n","    print(\"Number of Words: \" , len(nltk_tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YLACYiaxCKQx","executionInfo":{"status":"ok","timestamp":1644574097719,"user_tz":-330,"elapsed":585,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"59f3f7b3-8d5a-421b-e56e-b1a523a38f68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","['Summer', 'is', 'here', '.', 'Sky', 'is', 'bright', '.', 'Birds', 'are', 'gone', '.', 'Nests', 'are', 'empty', '.', 'Where', 'is', 'Rain', '?']\n","\n","\n","Number of Words:  20\n"]}]},{"cell_type":"code","source":["# Counting Words Using Split\n","# here the word 'here.' is counted as a single word and \n","# not 2 words as in case of using nltk.\n","with open('poem.txt', 'r') as file:\n","    lines_in_file = file.read()\n","    \n","    print( lines_in_file.split())\n","    print(\"\\n\")\n","print ( \"Number of Words: \", len(lines_in_file.split()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CM56y6MHC04D","executionInfo":{"status":"ok","timestamp":1644574153599,"user_tz":-330,"elapsed":365,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"42d1fb49-54aa-4ef5-ee39-c560e26e3970"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Summer', 'is', 'here.', 'Sky', 'is', 'bright.', 'Birds', 'are', 'gone.', 'Nests', 'are', 'empty.', 'Where', 'is', 'Rain?']\n","\n","\n","Number of Words:  15\n"]}]},{"cell_type":"markdown","source":["**Task - 5 : Filter Duplicate Words**"],"metadata":{"id":"jmsSHvXgDf2o"}},{"cell_type":"code","source":["# Without preserving the order\n","import nltk\n","word_data = \"The Sky is blue also the ocean is blue also Rainbow has a blue colour.\" \n","\n","# First Word tokenization\n","nltk_tokens = nltk.word_tokenize(word_data)\n","\n","# Applying Set\n","no_order = list(set(nltk_tokens))\n","\n","print (no_order)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgCaA5pGDkrS","executionInfo":{"status":"ok","timestamp":1644574312913,"user_tz":-330,"elapsed":405,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"52260494-7033-4a32-bf86-4603e66f69a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['the', 'has', 'Sky', '.', 'Rainbow', 'a', 'ocean', 'The', 'blue', 'colour', 'also', 'is']\n"]}]},{"cell_type":"code","source":["# Preserving the Order\n","import nltk\n","word_data = \"The Sky is blue also the ocean is blue also Rainbow has a blue colour.\" \n","# First Word tokenization\n","nltk_tokens = nltk.word_tokenize(word_data)\n","\n","ordered_tokens = set()\n","result = []\n","for word in nltk_tokens:\n","    if word not in ordered_tokens:\n","        ordered_tokens.add(word)\n","        result.append(word)\n","     \n","print( result  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUA_fIarEkgH","executionInfo":{"status":"ok","timestamp":1644571051580,"user_tz":-330,"elapsed":382,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"6f9352f5-510f-418c-897f-f839b25f8c34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['The', 'Sky', 'is', 'blue', 'also', 'the', 'ocean', 'Rainbow', 'has', 'a', 'colour', '.']\n"]}]},{"cell_type":"markdown","source":["**Task - 6 : Extract emails from TExt:**"],"metadata":{"id":"EHhRvLWgFAaB"}},{"cell_type":"code","source":["import re\n","text = \"Please contact us at contact@tutorialspoint.com for further information.\"+\\\n","        \" You can also give feedbacl at feedback@tp.com, d6ad.20-20@ves-it.ac.in\"\n","\n","\n","emails = re.findall(r\"[a-z0-9.\\-+_]+@[a-z0-9.\\-+_]+\\.[a-z]+\", text)\n","print (emails)"],"metadata":{"id":"kX-tOztpFF8m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Task - 7 : Extracting URL from Text**"],"metadata":{"id":"Ksd-m5hHFojC"}},{"cell_type":"code","source":["import re\n"," \n","with open(\"url_example.txt\") as file:\n","        for line in file:\n","            urls = re.findall('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', line)\n","            print(urls)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oi1h-3nzFtDi","executionInfo":{"status":"ok","timestamp":1644575150288,"user_tz":-330,"elapsed":399,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"6d310afa-b731-4bdf-9680-0f3fe8149f07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['http://www.google.com.']\n","['https://www.tutorialspoint.com']\n"]}]},{"cell_type":"markdown","source":["**Task - 8 : Pretty print Numbers**\n","* The python module **pprint** is used for giving proper printing formats to various data objects in python. \n","* Those data objects can represent a dictionary data type or even a data object containing the JSON data. "],"metadata":{"id":"EqQGj2mDF4QX"}},{"cell_type":"code","source":["import pprint\n","\n","student_dict = {'Name': 'Tusar', 'Class': 'XII', \n","     'Address': {'FLAT ':1308, 'BLOCK ':'A', 'LANE ':2, 'CITY ': 'HYD'}}\n","\n","print (student_dict)\n","print( \"\\n\")\n","print( \"***With Pretty Print***\")\n","print( \"-----------------------\")\n","pprint.pprint(student_dict,width=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ho0CZTkbGHW3","executionInfo":{"status":"ok","timestamp":1644571461276,"user_tz":-330,"elapsed":394,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"9a13a0fd-0c8a-4653-b442-17a3f7dc1d2e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Name': 'Tusar', 'Class': 'XII', 'Address': {'FLAT ': 1308, 'BLOCK ': 'A', 'LANE ': 2, 'CITY ': 'HYD'}}\n","\n","\n","***With Pretty Print***\n","-----------------------\n","{'Address': {'BLOCK ': 'A',\n","             'CITY ': 'HYD',\n","             'FLAT ': 1308,\n","             'LANE ': 2},\n"," 'Class': 'XII',\n"," 'Name': 'Tusar'}\n"]}]},{"cell_type":"code","source":["# Handling JSON Data\n","import pprint\n","\n","emp = {\"Name\":[\"Rick\",\"Dan\",\"Michelle\",\"Ryan\",\"Gary\",\"Nina\",\"Simon\",\"Guru\" ],\n","   \"Salary\":[\"623.3\",\"515.2\",\"611\",\"729\",\"843.25\",\"578\",\"632.8\",\"722.5\" ],   \n","   \"StartDate\":[ \"1/1/2012\",\"9/23/2013\",\"11/15/2014\",\"5/11/2014\",\"3/27/2015\",\"5/21/2013\",\n","      \"7/30/2013\",\"6/17/2014\"],\n","   \"Dept\":[ \"IT\",\"Operations\",\"IT\",\"HR\",\"Finance\",\"IT\",\"Operations\",\"Finance\"] }\n","\n","x= pprint.pformat(emp, indent=2)\n","print( x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c26wIDGEGVeP","executionInfo":{"status":"ok","timestamp":1644571499735,"user_tz":-330,"elapsed":365,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"05a75077-5fda-49d9-ab8d-9f5363e3eebc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{ 'Dept': [ 'IT',\n","            'Operations',\n","            'IT',\n","            'HR',\n","            'Finance',\n","            'IT',\n","            'Operations',\n","            'Finance'],\n","  'Name': ['Rick', 'Dan', 'Michelle', 'Ryan', 'Gary', 'Nina', 'Simon', 'Guru'],\n","  'Salary': ['623.3', '515.2', '611', '729', '843.25', '578', '632.8', '722.5'],\n","  'StartDate': [ '1/1/2012',\n","                 '9/23/2013',\n","                 '11/15/2014',\n","                 '5/11/2014',\n","                 '3/27/2015',\n","                 '5/21/2013',\n","                 '7/30/2013',\n","                 '6/17/2014']}\n"]}]},{"cell_type":"markdown","source":["**Task - 9 : Capitalize & Translate**"],"metadata":{"id":"ahKQrC9_GkNt"}},{"cell_type":"code","source":["help(string)"],"metadata":{"id":"eyRvjhWMHQRn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import string\n","\n","text = 'Tutorialspoint - simple easy learning.'\n","\n","print (string.capwords(text))\n","print (text.upper())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EfrXB1w2Go0_","executionInfo":{"status":"ok","timestamp":1644571694911,"user_tz":-330,"elapsed":364,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"0d017a3b-5953-4fc7-cde6-7bdc3efcb302"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tutorialspoint - Simple Easy Learning.\n","TUTORIALSPOINT - SIMPLE EASY LEARNING.\n"]}]},{"cell_type":"markdown","source":["* **Translation i**n python essentially means substituting specific letters with another letter. It can work for encryption decryption of strings."],"metadata":{"id":"rcUTomIyHhcI"}},{"cell_type":"code","source":["import string\n","\n","text = 'Tutorialspoint - simple easy learning.'\n","\n","transtable = text.maketrans('tpol', 'wxyz')\n","print (text.translate(transtable))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eCRLwzgQHiXY","executionInfo":{"status":"ok","timestamp":1644571826701,"user_tz":-330,"elapsed":378,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"66f30796-2b64-472c-c16d-d8e46661253d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tuwyriazsxyinw - simxze easy zearning.\n"]}]},{"cell_type":"markdown","source":["**Task - 10 : Text Translation**\n","* Text translation from one language to another is increasingly becoming common for various websites as they cater to an international audience. \n","* The python package which helps us do this is called **translate**"],"metadata":{"id":"TTsbmb6kH6qm"}},{"cell_type":"code","source":["!pip install translate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IC-0Ld9vIGEc","executionInfo":{"status":"ok","timestamp":1644571955427,"user_tz":-330,"elapsed":3482,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"eaae5fb8-213f-465c-ca6c-98da01352c89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting translate\n","  Downloading translate-3.6.1-py2.py3-none-any.whl (12 kB)\n","Collecting libretranslatepy==2.1.1\n","  Downloading libretranslatepy-2.1.1-py3-none-any.whl (3.2 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from translate) (7.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from translate) (4.2.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from translate) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->translate) (2.10)\n","Installing collected packages: libretranslatepy, translate\n","Successfully installed libretranslatepy-2.1.1 translate-3.6.1\n"]}]},{"cell_type":"code","source":["# translating a simple sentence from English to German. \n","# The default from language being English.\n","from translate import Translator\n","translator= Translator(to_lang=\"German\")\n","translation = translator.translate(\"Hello Friends!!\")\n","print (translation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2VLSgM1IJpC","executionInfo":{"status":"ok","timestamp":1644575482908,"user_tz":-330,"elapsed":1077,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"5007d350-1767-4104-a633-6c0aee5dd806"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Hallo Freunde\n"]}]},{"cell_type":"code","source":["from translate import Translator\n","translator= Translator(from_lang=\"german\",to_lang=\"marathi\")\n","translation = translator.translate(\"Guten Morgen\")\n","print (translation)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xDBRQ0SjIdie","executionInfo":{"status":"ok","timestamp":1644572144949,"user_tz":-330,"elapsed":1054,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"5e9d2f1c-25ab-4dd5-f38a-11ec3449ec6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["शुभ प्रभात\n"]}]},{"cell_type":"markdown","source":["# **Phases of Text Processing**\n","* Text processing contains two main phases, which are **tokenization and normalization**. \n","\n","> **Tokenization** is the process of splitting a longer string of text into smaller pieces, or tokens \n","\n","> **Normalization** referring to convert number to their word equivalent, remove punctuation, convert all text to the same case, remove stopwords, remove noise, lemmatizing and stemming.\n","\n","* **Stemming** — removing affixes (suffixed, prefixes, infixes, circumfixes), For example, running to run\n","\n","* **Lemmatization** — capture canonical form based on a word’s lemma. For example, better to good"],"metadata":{"id":"Dooco0N9O6eY"}},{"cell_type":"markdown","source":["**Text Processing with NLTK**"],"metadata":{"id":"xLPQQ88mTF1D"}},{"cell_type":"code","source":["text = \"\"\"The idea of giving computers the ability to process human language is\n"," as old as the idea of computers themselves. This book is about the \n"," implementation and implications of that exciting idea. We introduce a vibrant \n"," interdisciplinary field with many names corresponding to its many facets, names\n","  like speech and language processing, human language technology, \n","  natural language processing, computational linguistics, and speech recognition\n","   and synthesis. The goal of this new field is to get computers to perform \n","   useful tasks involving human language, tasks like enabling human-machine\n","    communication, improving human-human communication, or simply doing useful \n","    processing of text or speech.\"\"\""],"metadata":{"id":"EeQI8ciiS25h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import WordPunctTokenizer\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","# needed for nltk.pos_tag function nltk.download(’averaged_perceptron_tagger’)\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ryyBdIFxTJwh","executionInfo":{"status":"ok","timestamp":1644576017085,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"74d6d598-6432-40ee-9225-e9e73e5b2207"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"markdown","source":["**Step 1 : Tokenization**\n","* Using tokenizer to separate the sentences into a list of single words (tokens).\n"],"metadata":{"id":"xB3NoRJjThbb"}},{"cell_type":"code","source":["word_punct_token = WordPunctTokenizer().tokenize(text)\n","print(word_punct_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NCYBoaHDTnGg","executionInfo":{"status":"ok","timestamp":1644576035145,"user_tz":-330,"elapsed":356,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"4931444b-c994-4213-ee1f-a36b96b2b8d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Please', 'contact', 'us', 'at', 'contact', '@', 'tutorialspoint', '.', 'com', 'for', 'further', 'information', '.', 'You', 'can', 'also', 'give', 'feedbacl', 'at', 'feedback', '@', 'tp', '.', 'com', ',', 'd6ad', '.', '20', '-', '20', '@', 'ves', '-', 'it', '.', 'ac', '.', 'in']\n"]}]},{"cell_type":"markdown","source":["* There are several tokenizer modules in NLTK libraries\n","* Eg : **RegexpTokenizer** able to separate the currency like $9.99 as a single token by setting ***RegexpTokenizer(‘\\w+|\\$[\\d\\.]+|\\S+’)***\n","* All the tokenizers mentioned will be return the tokens in the list form. \n","* NLTK also have a module name sent_tokenize which able to separate paragraphs into the list of sentences."],"metadata":{"id":"43pU9qipT4el"}},{"cell_type":"markdown","source":["**Step 2 : Normalization**\n","* The script below removed the tokens which are not a word, for example, the symbols and numbers, also tokens that only contain less than two letters or contain only consonants.\n","* This script might not be useful in this example, but it’s pretty useful when you are dealing with a large set of text data, it helps to clean up much noise."],"metadata":{"id":"Up3Pbxb0URzb"}},{"cell_type":"code","source":["clean_token=[]\n","for token in word_punct_token:\n","    token = token.lower()\n","    # remove any value that are not alphabetical\n","    new_token = re.sub(r'[^a-zA-Z]+', '', token) \n","    # remove empty value and single character value\n","    if new_token != \"\" and len(new_token) >= 2: \n","        vowels=len([v for v in new_token if v in \"aeiou\"])\n","        if vowels != 0: # remove line that only contains consonants\n","            clean_token.append(new_token)\n","print(clean_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f75kih8iUc58","executionInfo":{"status":"ok","timestamp":1644576038649,"user_tz":-330,"elapsed":7,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"3934e227-70dd-4b80-cda6-8c4797c5c8ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['please', 'contact', 'us', 'at', 'contact', 'tutorialspoint', 'com', 'for', 'further', 'information', 'you', 'can', 'also', 'give', 'feedbacl', 'at', 'feedback', 'com', 'dad', 'ves', 'it', 'ac', 'in']\n"]}]},{"cell_type":"markdown","source":["**Step 2a. Removing Stopwords**\n","* Stopwords referring to the word which does not carry much insight, such as preposition. \n","* NLTK and spaCy have a different amount of stopwords in the library, but both NLTK and spaCy allow us to add in any word we feel necessary. \n","* For example, when we dealing with email, we may add in ***gmail, com, outlook*** as stopwords."],"metadata":{"id":"XkfpKAMnUorA"}},{"cell_type":"code","source":["# Get the list of stop words\n","stop_words = stopwords.words('english')\n","# add new stopwords to the list\n","stop_words.extend([\"could\",\"though\",\"would\",\"also\",\"many\",'much','us'])\n","print(stop_words)\n","# Remove the stopwords from the list of tokens\n","tokens = [x for x in clean_token if x not in stop_words]\n","print(tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwpjkr4rU0pL","executionInfo":{"status":"ok","timestamp":1644576069888,"user_tz":-330,"elapsed":397,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"15362bc5-bca2-4478-be14-4ac0bd2369ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'could', 'though', 'would', 'also', 'many', 'much', 'us']\n","['please', 'contact', 'contact', 'tutorialspoint', 'com', 'information', 'give', 'feedbacl', 'feedback', 'com', 'dad', 'ves', 'ac']\n"]}]},{"cell_type":"markdown","source":["**Step 2b. Lemmatization**\n","* Lemmatizing and stemming both help to ***reduce the dimension of the vocabulary by return the words to their root form*** (lemmatizing) or remove all the suffix, affix, prefix and so on.\n","* Stemming is nice for reducing the dimension of vocabulary, but most of the time the word become meaningless as stemming only chopped off the suffix but not returning the words to their base form. \n","* For example, houses --> hous after stemming, which completely lose its meaning. \n","* Hence, lemmatizing is more preferable for text analytics."],"metadata":{"id":"yHXMghQNWNZs"}},{"cell_type":"code","source":["# Create lemmatizer object \n","lemmatizer = WordNetLemmatizer()\n","# Lemmatize each word and display the output\n","lemmatize_text = []\n","for word in tokens:\n","    output = [word, lemmatizer.lemmatize(word, pos='n'),\n","              lemmatizer.lemmatize(word, pos='a'),\n","              lemmatizer.lemmatize(word, pos='v')]\n","    lemmatize_text.append(output)\n","print(lemmatize_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TK8Cvnq9WjJH","executionInfo":{"status":"ok","timestamp":1644475178918,"user_tz":-330,"elapsed":1450,"user":{"displayName":"Lifna Jos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07993679914089209728"}},"outputId":"d1ed2ddd-760f-43a1-8d3a-cbe306d4cfb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['idea', 'idea', 'idea', 'idea'], ['giving', 'giving', 'giving', 'give'], ['computers', 'computer', 'computers', 'computers'], ['ability', 'ability', 'ability', 'ability'], ['process', 'process', 'process', 'process'], ['human', 'human', 'human', 'human'], ['language', 'language', 'language', 'language'], ['old', 'old', 'old', 'old'], ['idea', 'idea', 'idea', 'idea'], ['computers', 'computer', 'computers', 'computers'], ['book', 'book', 'book', 'book'], ['implementation', 'implementation', 'implementation', 'implementation'], ['implications', 'implication', 'implications', 'implications'], ['exciting', 'exciting', 'exciting', 'excite'], ['idea', 'idea', 'idea', 'idea'], ['introduce', 'introduce', 'introduce', 'introduce'], ['vibrant', 'vibrant', 'vibrant', 'vibrant'], ['interdisciplinary', 'interdisciplinary', 'interdisciplinary', 'interdisciplinary'], ['field', 'field', 'field', 'field'], ['names', 'name', 'names', 'name'], ['corresponding', 'corresponding', 'corresponding', 'correspond'], ['facets', 'facet', 'facets', 'facets'], ['names', 'name', 'names', 'name'], ['like', 'like', 'like', 'like'], ['speech', 'speech', 'speech', 'speech'], ['language', 'language', 'language', 'language'], ['processing', 'processing', 'processing', 'process'], ['human', 'human', 'human', 'human'], ['language', 'language', 'language', 'language'], ['technology', 'technology', 'technology', 'technology'], ['natural', 'natural', 'natural', 'natural'], ['language', 'language', 'language', 'language'], ['processing', 'processing', 'processing', 'process'], ['computational', 'computational', 'computational', 'computational'], ['linguistics', 'linguistics', 'linguistics', 'linguistics'], ['speech', 'speech', 'speech', 'speech'], ['recognition', 'recognition', 'recognition', 'recognition'], ['synthesis', 'synthesis', 'synthesis', 'synthesis'], ['goal', 'goal', 'goal', 'goal'], ['new', 'new', 'new', 'new'], ['field', 'field', 'field', 'field'], ['get', 'get', 'get', 'get'], ['computers', 'computer', 'computers', 'computers'], ['perform', 'perform', 'perform', 'perform'], ['useful', 'useful', 'useful', 'useful'], ['tasks', 'task', 'tasks', 'task'], ['involving', 'involving', 'involving', 'involve'], ['human', 'human', 'human', 'human'], ['language', 'language', 'language', 'language'], ['tasks', 'task', 'tasks', 'task'], ['like', 'like', 'like', 'like'], ['enabling', 'enabling', 'enabling', 'enable'], ['human', 'human', 'human', 'human'], ['machine', 'machine', 'machine', 'machine'], ['communication', 'communication', 'communication', 'communication'], ['improving', 'improving', 'improving', 'improve'], ['human', 'human', 'human', 'human'], ['human', 'human', 'human', 'human'], ['communication', 'communication', 'communication', 'communication'], ['simply', 'simply', 'simply', 'simply'], ['useful', 'useful', 'useful', 'useful'], ['processing', 'processing', 'processing', 'process'], ['text', 'text', 'text', 'text'], ['speech', 'speech', 'speech', 'speech']]\n"]}]},{"cell_type":"markdown","source":["# **Digital material:**"],"metadata":{"id":"R5fGrIQNfON1"}},{"cell_type":"markdown","source":["1. https://towardsdatascience.com/text-processing-in-python-29e86ea4114c\n","\n","2. https://www.tutorialspoint.com/python_text_processing/python_capitalize_and_translate.htm\n","\n"," "],"metadata":{"id":"vlxt8s_K3dYt"}}]}